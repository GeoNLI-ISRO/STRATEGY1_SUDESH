{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tritask_with_dino.py\n",
    "\"\"\"\n",
    "Tri-Task training script (BLIP-2 + LoRA) + partial Grounding-DINO-L fine-tune (last 2 blocks)\n",
    "- Dataset: expects per-image JSON annotations in train_annotations/ and validation_annotations/\n",
    "- Uses Hungarian matching for grounding supervision (DETR-style)\n",
    "- Designed for 2x A100-40GB with bf16 + DeepSpeed/ZeRO\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.ops as tvops\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# === USER SETTINGS ===\n",
    "TRAIN_IMAGES_DIR = \"/path/to/train_images\"\n",
    "VAL_IMAGES_DIR   = \"/path/to/validation_images\"\n",
    "TRAIN_ANNO_DIR   = \"/path/to/train_annotations\"\n",
    "VAL_ANNO_DIR     = \"/path/to/validation_annotations\"\n",
    "OUTPUT_DIR = \"./checkpoints_tritask\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model & training hyperparams\n",
    "EPOCHS = 6                      # 4 LoRA warmup + 2 partial DINO fine-tune\n",
    "MICRO_BATCH = 4                 # per GPU\n",
    "GRAD_ACCUM = 2\n",
    "GLOBAL_BATCH = MICRO_BATCH * GRAD_ACCUM * 2  # 2 GPUs by default (adjust)\n",
    "LR_LORA = 2e-5\n",
    "LR_DINO = 5e-6                  # small lr for DINO last-blocks\n",
    "WEIGHT_DECAY = 0.01\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "NUM_DINO_UNFREEZE_BLOCKS = 2    # last N blocks of DINO to unfreeze\n",
    "NUM_QUERIES = 100\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_SIZE = 640\n",
    "\n",
    "# DeepSpeed: point to config file if you plan to use deepspeed\n",
    "DEEPSPEED = True\n",
    "DEEPSPEED_CONFIG = \"deepspeed_config.json\"\n",
    "\n",
    "# ======================================================\n",
    "#  Utility: Hungarian matcher (DETR-style)\n",
    "# ======================================================\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    cx, cy, w, h = x[..., 0], x[..., 1], x[..., 2], x[..., 3]\n",
    "    x0 = cx - 0.5 * w\n",
    "    y0 = cy - 0.5 * h\n",
    "    x1 = cx + 0.5 * w\n",
    "    y1 = cy + 0.5 * h\n",
    "    return torch.stack([x0, y0, x1, y1], dim=-1)\n",
    "\n",
    "def box_area(boxes):\n",
    "    return (boxes[..., 2] - boxes[..., 0]).clamp(min=0) * (boxes[..., 3] - boxes[..., 1]).clamp(min=0)\n",
    "\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    # boxes in xyxy\n",
    "    # adapt from torchvision.ops.generalized_box_iou\n",
    "    return tvops.generalized_box_iou(boxes1, boxes2)\n",
    "\n",
    "class HungarianMatcher:\n",
    "    def __init__(self, cost_class=1, cost_bbox=5, cost_giou=2):\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def match(self, outputs_logits, outputs_boxes, targets):\n",
    "        # outputs_logits: (B, Q, C)  (here C=1 if objectness or >1 if classes)\n",
    "        # outputs_boxes: (B, Q, 4) in normalized cxcywh\n",
    "        # targets: list of dicts with 'boxes' in normalized xyxy or cxcywh\n",
    "        bs, q = outputs_boxes.shape[0], outputs_boxes.shape[1]\n",
    "        indices = []\n",
    "        for b in range(bs):\n",
    "            tgt_boxes = targets[b]  # tensor (G,4) in xyxy\n",
    "            if tgt_boxes.numel() == 0:\n",
    "                indices.append((np.array([], dtype=np.int64), np.array([], dtype=np.int64)))\n",
    "                continue\n",
    "            # class cost (objectness logits -> prob)\n",
    "            out_prob = torch.sigmoid(outputs_logits[b]).cpu().numpy()  # (Q,) or (Q,C)\n",
    "            # if multi-class, you would use negative log-probs for GT class\n",
    "            # bbox cost: L1 between boxes\n",
    "            out_boxes_xyxy = box_cxcywh_to_xyxy(outputs_boxes[b]).cpu()\n",
    "            tgt_boxes_xyxy = tgt_boxes.cpu()\n",
    "            # L1 cost\n",
    "            cost_bbox = torch.cdist(outputs_boxes[b], ( ( (tgt_boxes_xyxy[...,0]+tgt_boxes_xyxy[...,2])/2).unsqueeze(1) if False else tgt_boxes_xyxy ), p=1).cpu().numpy()\n",
    "            # Simpler: use L1 on cxcywh (if targets provided as cxcywh)\n",
    "            # Use GIoU cost\n",
    "            # ensure shapes\n",
    "            qboxes = out_boxes_xyxy\n",
    "            gious = generalized_box_iou(qboxes, tgt_boxes_xyxy).cpu().numpy()  # (Q,G)\n",
    "            cost_giou = -gious\n",
    "            # combine costs (class not used much since single class)\n",
    "            C = self.cost_bbox * cost_bbox + self.cost_giou * cost_giou\n",
    "            # Hungarian\n",
    "            q_idx, t_idx = linear_sum_assignment(C)\n",
    "            indices.append((q_idx.astype(np.int64), t_idx.astype(np.int64)))\n",
    "        return indices\n",
    "\n",
    "# ======================================================\n",
    "# Dataset class\n",
    "# ======================================================\n",
    "class TriTaskDataset(Dataset):\n",
    "    def __init__(self, image_dir, anno_dir, processor, img_size=IMG_SIZE, mode=\"train\"):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.anno_dir = Path(anno_dir)\n",
    "        self.processor = processor\n",
    "        self.img_size = img_size\n",
    "        self.files = sorted([p for p in self.anno_dir.glob(\"*.json\")])\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.files[idx]\n",
    "        with open(p, \"r\") as f:\n",
    "            ann = json.load(f)\n",
    "        img_path = self.image_dir / ann[\"image\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize((self.img_size, self.img_size))\n",
    "        pixel_values = self.processor(images=img, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        caption = ann.get(\"caption\", \"\")\n",
    "        qa_pairs = ann.get(\"qa_pairs\", [])\n",
    "        qa = random.choice(qa_pairs) if len(qa_pairs) > 0 else {\"question\":\"\", \"answer\":\"\"}\n",
    "        # objects: list of dicts with obj_coord [x1,y1,x2,y2] normalized\n",
    "        objs = ann.get(\"objects\", [])\n",
    "        if len(objs) == 0:\n",
    "            boxes = torch.zeros((0,4), dtype=torch.float32)\n",
    "        else:\n",
    "            coords = [o.get(\"obj_coord\", [0,0,1,1]) for o in objs]\n",
    "            # convert xyxy normalized [x1,y1,x2,y2] to cxcywh normalized for matching/pred\n",
    "            arr = np.array(coords, dtype=np.float32)\n",
    "            x1, y1, x2, y2 = arr[:,0], arr[:,1], arr[:,2], arr[:,3]\n",
    "            cx = (x1 + x2) / 2.0\n",
    "            cy = (y1 + y2) / 2.0\n",
    "            w = (x2 - x1)\n",
    "            h = (y2 - y1)\n",
    "            cxcywh = np.stack([cx, cy, w, h], axis=1)\n",
    "            boxes = torch.tensor(cxcywh, dtype=torch.float32)\n",
    "        return {\"pixel_values\": pixel_values, \"caption\": caption, \"question\": qa.get(\"question\",\"\"), \"answer\": qa.get(\"answer\",\"\"), \"boxes\": boxes}\n",
    "\n",
    "# ======================================================\n",
    "# Load models: Grounding-DINO + BLIP-2 with LoRA\n",
    "# ======================================================\n",
    "def load_grounding_dino(model_config_path: str, checkpoint_path: str):\n",
    "    \"\"\"\n",
    "    Load GroundingDINO model from repository. You must have GroundingDINO installed as a package.\n",
    "    This function is written for the GroundingDINO repo structure. Adjust import paths if needed.\n",
    "    \"\"\"\n",
    "    # Import inside function to avoid module error if repo not installed\n",
    "    from groundingdino.util.slconfig import SLConfig\n",
    "    from groundingdino.models import build_model\n",
    "    cfg = SLConfig.fromfile(model_config_path)\n",
    "    model = build_model(cfg)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "    return model\n",
    "\n",
    "print(\"Loading BLIP-2 processor & model (this will download weights if not cached)...\")\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "blip2 = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "# inject LoRA adapters\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"q_attention\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "blip2 = get_peft_model(blip2, lora_cfg)\n",
    "\n",
    "# Grounding-DINO config / checkpoint paths (edit these)\n",
    "DINO_CONFIG = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"  # example path in repo\n",
    "DINO_CHECKPOINT = \"/path/to/groundingdino_swint_ogc.pth\"\n",
    "\n",
    "print(\"Loading Grounding-DINO model (may take time)...\")\n",
    "grounding_dino = load_grounding_dino(DINO_CONFIG, DINO_CHECKPOINT)\n",
    "grounding_dino.to(DEVICE)\n",
    "grounding_dino.eval()  # we'll set train mode later conditionally\n",
    "\n",
    "# ======================================================\n",
    "# Partial unfreeze: unfreeze last N transformer blocks and head\n",
    "# ======================================================\n",
    "def unfreeze_last_blocks_dino(model, n_blocks=2):\n",
    "    \"\"\"\n",
    "    This inspects grounding_dino backbone to find transformer blocks and unfreezes last n_blocks.\n",
    "    Implementation depends on model architecture; this works for SwinT-like backbones in GroundingDINO.\n",
    "    \"\"\"\n",
    "    # freeze all first\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # try to find backbone blocks attr names\n",
    "    unfreezed = 0\n",
    "    # attempt several likely attribute paths (depends on repo version)\n",
    "    candidate_paths = [\n",
    "        \"backbone.model.blocks\",      # possible path\n",
    "        \"backbone.blocks\",            # possible path\n",
    "        \"backbone.stage4.blocks\",     # possible path\n",
    "        \"backbone.model.stage4.blocks\"\n",
    "    ]\n",
    "    found = False\n",
    "    for path in candidate_paths:\n",
    "        try:\n",
    "            # resolve attribute\n",
    "            obj = model\n",
    "            for part in path.split(\".\"):\n",
    "                obj = getattr(obj, part)\n",
    "            blocks = obj\n",
    "            # unfreeze last n_blocks\n",
    "            for i in range(-n_blocks, 0):\n",
    "                for p in blocks[i].parameters():\n",
    "                    p.requires_grad = True\n",
    "            found = True\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    # always unfreeze the head (box predictor / text encoder if exists)\n",
    "    # best effort: look for 'box_head' or 'head' attributes\n",
    "    for name, module in model.named_modules():\n",
    "        if \"head\" in name.lower() or \"predictor\" in name.lower() or \"out\" in name.lower():\n",
    "            for p in module.parameters():\n",
    "                p.requires_grad = True\n",
    "    if not found:\n",
    "        print(\"Warning: didn't find canonical backbone block path to unfreeze; you must manually set blocks to unfreeze.\")\n",
    "    else:\n",
    "        print(f\"Unfroze last {n_blocks} DINO blocks (if path resolution succeeded).\")\n",
    "\n",
    "# call unfreeze (we will unfreeze at correct stage in training loop)\n",
    "unfreeze_last_blocks_dino(grounding_dino, n_blocks=NUM_DINO_UNFREEZE_BLOCKS)\n",
    "\n",
    "# ======================================================\n",
    "# Build head & wrapper utilities (we use DINO's own head for proposals during training)\n",
    "# ======================================================\n",
    "\n",
    "# For grounding training we will use DINO's internal forward to get predictions (logits + boxes).\n",
    "# The exact forward signature depends on repo; below we use the common pattern found in their inference:\n",
    "#  - backbone -> features\n",
    "#  - transformer -> outputs\n",
    "#  - head -> pred logits + boxes\n",
    "# So we'll call model with images and training flag to get raw outputs.\n",
    "\n",
    "# ======================================================\n",
    "# Loss functions & criterion helpers\n",
    "# ======================================================\n",
    "class TriTaskCriterion:\n",
    "    def __init__(self, weight_dict=None):\n",
    "        # weights for losses\n",
    "        self.weight_dict = weight_dict or {\"caption\":1.0, \"ground\":1.0, \"vqa\":1.0}\n",
    "        self.matcher = HungarianMatcher(cost_bbox=5, cost_giou=2)\n",
    "        self.l1 = nn.L1Loss(reduction=\"none\")\n",
    "\n",
    "    def loss_caption(self, blip_outputs_loss):\n",
    "        return blip_outputs_loss\n",
    "\n",
    "    def loss_vqa(self, vqa_loss):\n",
    "        return vqa_loss\n",
    "\n",
    "    def loss_grounding(self, pred_logits, pred_boxes, targets_boxes):\n",
    "        \"\"\"\n",
    "        pred_logits: (B, Q) objectness logits (if multi-class use shape (B,Q,C))\n",
    "        pred_boxes: (B, Q, 4) cxcywh normalized\n",
    "        targets_boxes: list of tensors (G,4) xyxy normalized (we'll convert)\n",
    "        Returns scalar loss = cls_loss + bbox_l1 + giou_loss\n",
    "        \"\"\"\n",
    "        # Convert targets to xyxy for giou\n",
    "        targets_xyxy = []\n",
    "        for tb in targets_boxes:\n",
    "            if tb.numel() == 0:\n",
    "                targets_xyxy.append(torch.zeros((0,4), device=pred_boxes.device))\n",
    "            else:\n",
    "                # we have targets in cxcywh? Our dataset is cxcywh -> convert to xyxy\n",
    "                cx = tb[:,0]; cy = tb[:,1]; w = tb[:,2]; h = tb[:,3]\n",
    "                x1 = cx - 0.5*w; y1 = cy - 0.5*h; x2 = cx + 0.5*w; y2 = cy + 0.5*h\n",
    "                targets_xyxy.append(torch.stack([x1,y1,x2,y2], dim=1))\n",
    "\n",
    "        # matching\n",
    "        indices = self.matcher.match(pred_logits, pred_boxes, targets_xyxy)\n",
    "        batch_loss_bbox = []\n",
    "        batch_loss_obj = []\n",
    "        batch_loss_giou = []\n",
    "\n",
    "        B, Q, _ = pred_boxes.shape\n",
    "        for b in range(B):\n",
    "            q_idx, t_idx = indices[b]\n",
    "            if len(t_idx) == 0:\n",
    "                # no targets, encourage low objectness\n",
    "                obj_loss = torch.mean(torch.sigmoid(pred_logits[b]))\n",
    "                batch_loss_obj.append(obj_loss)\n",
    "                batch_loss_bbox.append(torch.tensor(0.0, device=pred_boxes.device))\n",
    "                batch_loss_giou.append(torch.tensor(0.0, device=pred_boxes.device))\n",
    "                continue\n",
    "\n",
    "            src_boxes = pred_boxes[b][q_idx]    # (G,4) cxcywh\n",
    "            tgt_boxes = targets_boxes[b]       # (G,4) cxcywh\n",
    "            # bbox L1 on cxcywh\n",
    "            l1 = torch.abs(src_boxes - tgt_boxes).mean()\n",
    "            # giou on xyxy\n",
    "            src_xy = box_cxcywh_to_xyxy(src_boxes)\n",
    "            tgt_xy = box_cxcywh_to_xyxy(tgt_boxes)\n",
    "            giou = tvops.generalized_box_iou(src_xy, tgt_xy)\n",
    "            giou_loss = (1 - torch.diag(giou)).mean()\n",
    "            # objectness loss: positive for matched preds, negative for others\n",
    "            obj_targets = torch.zeros(Q, device=pred_logits.device)\n",
    "            obj_targets[q_idx] = 1.0\n",
    "            obj_loss = nn.functional.binary_cross_entropy_with_logits(pred_logits[b], obj_targets)\n",
    "            batch_loss_bbox.append(l1)\n",
    "            batch_loss_obj.append(obj_loss)\n",
    "            batch_loss_giou.append(giou_loss)\n",
    "        loss_bbox = torch.stack(batch_loss_bbox).mean()\n",
    "        loss_obj = torch.stack(batch_loss_obj).mean()\n",
    "        loss_giou = torch.stack(batch_loss_giou).mean()\n",
    "        return loss_obj + 5.0*loss_bbox + 2.0*loss_giou\n",
    "\n",
    "# ======================================================\n",
    "# Data loaders\n",
    "# ======================================================\n",
    "train_dataset = TriTaskDataset(TRAIN_IMAGES_DIR, TRAIN_ANNO_DIR, processor, img_size=IMG_SIZE, mode=\"train\")\n",
    "val_dataset   = TriTaskDataset(VAL_IMAGES_DIR, VAL_ANNO_DIR, processor, img_size=IMG_SIZE, mode=\"val\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=MICRO_BATCH, shuffle=True, num_workers=6, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=MICRO_BATCH, shuffle=False, num_workers=6, pin_memory=True)\n",
    "\n",
    "# ======================================================\n",
    "# Optimizer groups: LoRA params + small heads + DINO last blocks (if any)\n",
    "# ======================================================\n",
    "# collect trainable params\n",
    "def get_trainable_params(blip2_model, dino_model):\n",
    "    params = []\n",
    "    # LoRA adapters in blip2 are trainable (peft manages it), include all model.parameters() and later filter\n",
    "    for name, p in blip2_model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            params.append(p)\n",
    "    # include DINO trainable params\n",
    "    for name, p in dino_model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            params.append(p)\n",
    "    return params\n",
    "\n",
    "trainable_params = get_trainable_params(blip2, grounding_dino)\n",
    "optimizer = optim.AdamW(trainable_params, lr=LR_LORA, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# set a smaller LR for DINO params\n",
    "# (we will use param groups to set LR for all params, then for DINO params set LR_DINO)\n",
    "dino_param_ids = {id(p) for n,p in grounding_dino.named_parameters() if p.requires_grad}\n",
    "param_groups = [\n",
    "    {\"params\": [p for p in trainable_params if id(p) not in dino_param_ids], \"lr\": LR_LORA},\n",
    "    {\"params\": [p for p in trainable_params if id(p) in dino_param_ids], \"lr\": LR_DINO}\n",
    "]\n",
    "optimizer = optim.AdamW(param_groups, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = math.ceil(len(train_loader) * EPOCHS / GRAD_ACCUM)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps)\n",
    "\n",
    "criterion = TriTaskCriterion()\n",
    "\n",
    "# ======================================================\n",
    "# Training loop\n",
    "# ======================================================\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "global_step = 0\n",
    "\n",
    "print(f\"Training on device={DEVICE} -- total_steps approx {total_steps}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    blip2.train()\n",
    "    # set DINO train/eval: we kept only last N blocks trainable; put model.train() to update them\n",
    "    grounding_dino.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        pv = batch[\"pixel_values\"].to(DEVICE)             # (B,C,H,W)\n",
    "        captions = batch[\"caption\"]\n",
    "        questions = batch[\"question\"]\n",
    "        answers = batch[\"answer\"]\n",
    "        target_boxes = batch[\"boxes\"].to(DEVICE)          # (B, G, 4) cxcywh; may be zero-length\n",
    "\n",
    "        # choose task by sampling: caption 40%, grounding 40%, vqa 20%\n",
    "        r = random.random()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            if r < 0.4:\n",
    "                # CAPTION\n",
    "                inputs = processor(images=pv, text=[\"\"]*pv.size(0), return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "                outputs = blip2(**inputs, labels=inputs.input_ids)\n",
    "                loss = criterion.loss_caption(outputs.loss)\n",
    "            elif r < 0.8:\n",
    "                # GROUNDING: use DINO forward to get objectness logits and boxes\n",
    "                # DINO forward depends on repo API; the following is a best-effort interface:\n",
    "                dino_inputs = {\"image\": pv}  # if repo requires preprocessing, replace accordingly\n",
    "                dino_out = grounding_dino.forward(pv)  # replace with correct forward signature if different\n",
    "                # extract logits & boxes: preferences depend on repo; typical outputs: {\"pred_logits\":..., \"pred_boxes\":...}\n",
    "                # here we attempt to access attributes; adapt if your groundingdino version differs\n",
    "                try:\n",
    "                    pred_logits = dino_out[\"pred_logits\"]   # (B, Q, C) or (B, Q) for objectness\n",
    "                    pred_boxes = dino_out[\"pred_boxes\"]     # (B, Q, 4) in cxcywh normalized\n",
    "                except Exception:\n",
    "                    # fallback: many repo versions return dict with keys\n",
    "                    pred_logits = dino_out.pred_logits\n",
    "                    pred_boxes = dino_out.pred_boxes\n",
    "                # ensure shapes\n",
    "                if pred_logits.ndim == 3 and pred_logits.shape[-1] > 1:\n",
    "                    # multi-class; reduce to objectness for simple case by taking max class logits\n",
    "                    pred_logits_obj = torch.max(pred_logits, dim=-1)[0]\n",
    "                else:\n",
    "                    pred_logits_obj = pred_logits.squeeze(-1) if pred_logits.ndim==3 else pred_logits\n",
    "                # prepare targets: list of tensors in cxcywh (we already have)\n",
    "                targets_list = []\n",
    "                for b in range(target_boxes.shape[0]):\n",
    "                    targets_list.append(target_boxes[b])\n",
    "                loss = criterion.loss_grounding(pred_logits_obj, pred_boxes, targets_list)\n",
    "            else:\n",
    "                # VQA\n",
    "                # prepare inputs: use question prompt\n",
    "                vqa_losses = []\n",
    "                for i in range(pv.size(0)):\n",
    "                    q = questions[i]\n",
    "                    a = answers[i]\n",
    "                    if (not q) or (not a):\n",
    "                        # fallback to caption for this sample\n",
    "                        inputs = processor(images=pv[i].unsqueeze(0), text=\"\", return_tensors=\"pt\").to(DEVICE)\n",
    "                        out = blip2(**inputs, labels=inputs.input_ids)\n",
    "                        vqa_losses.append(out.loss)\n",
    "                    else:\n",
    "                        inputs = processor(images=pv[i].unsqueeze(0), text=q, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "                        labels = processor.tokenizer(a, return_tensors=\"pt\", padding=True).input_ids.to(DEVICE)\n",
    "                        out = blip2(**inputs, labels=labels)\n",
    "                        vqa_losses.append(out.loss)\n",
    "                loss = torch.stack(vqa_losses).mean()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if (batch_idx + 1) % GRAD_ACCUM == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_( [p for group in optimizer.param_groups for p in group['params'] if p.requires_grad], 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": running_loss / (batch_idx + 1), \"step\": global_step})\n",
    "\n",
    "    # Save checkpoint (model + peft adapters)\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch+1,\n",
    "        \"global_step\": global_step,\n",
    "        \"model_state_dict\": blip2.state_dict(),\n",
    "        \"dino_state_dict\": grounding_dino.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(ckpt, os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "    # Save PEFT adapters separately (if peft supports save_pretrained)\n",
    "    try:\n",
    "        blip2.save_pretrained(os.path.join(OUTPUT_DIR, f\"blip2_peft_epoch_{epoch+1}\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"Epoch {epoch+1} checkpoint saved.\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_image(image_pil, task=\"caption\", question=None, blip2_model=None, grounding_dino_model=None, peft_adapter_dir=None):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if peft_adapter_dir:\n",
    "        blip2_model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "        blip2_model = PeftModel.from_pretrained(blip2_model, peft_adapter_dir).to(device)\n",
    "    blip2_model.to(device).eval()\n",
    "    grounding_dino_model.to(device).eval()\n",
    "\n",
    "    if task == \"caption\":\n",
    "        inputs = processor(images=image_pil, return_tensors=\"pt\").to(device)\n",
    "        gen = blip2_model.generate(**inputs, max_new_tokens=80)\n",
    "        caption = processor.decode(gen[0], skip_special_tokens=True)\n",
    "        return {\"caption\": caption}\n",
    "\n",
    "    if task == \"qa\":\n",
    "        if question is None:\n",
    "            return {\"error\": \"Question required\"}\n",
    "        inputs = processor(images=image_pil, text=question, return_tensors=\"pt\").to(device)\n",
    "        gen = blip2_model.generate(**inputs, max_new_tokens=40)\n",
    "        ans = processor.decode(gen[0], skip_special_tokens=True)\n",
    "        return {\"answer\": ans}\n",
    "\n",
    "    if task == \"yesno\":\n",
    "        if question is None:\n",
    "            return {\"error\": \"Question required\"}\n",
    "        q = question + \" Answer yes or no.\"\n",
    "        inputs = processor(images=image_pil, text=q, return_tensors=\"pt\").to(device)\n",
    "        gen = blip2_model.generate(**inputs, max_new_tokens=8)\n",
    "        ans = processor.decode(gen[0], skip_special_tokens=True).lower()\n",
    "        return {\"answer\": \"Yes\" if \"yes\" in ans else \"No\"}\n",
    "\n",
    "    if task == \"grounding\":\n",
    "        # GroundingDINO inference utilities usually provide predict_with_caption()\n",
    "        # Example: boxes, logits, phrases = grounding_dino.predict_with_caption(image_pil, caption=\"object\", box_threshold=0.3)\n",
    "        # adapt to your repo's function name\n",
    "        boxes, logits, phrases = grounding_dino_model.predict_with_caption(image_pil, caption=\"object\", box_threshold=0.35, text_threshold=0.25)\n",
    "        objects = []\n",
    "        for b, p in zip(boxes, phrases):\n",
    "            x1,y1,x2,y2 = b.tolist()\n",
    "            objects.append({\"obj_cls\": p, \"obj_coord\":[x1,y1,x2,y2]})\n",
    "        return {\"objects\": objects}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
